[
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "print('Hello World!')\n\nHello World!\n\n\nThey say that the best person to learn from is someone that is only a few steps ahead of you in their own learning journey…\nIn the subject of blogging I found Christian’s blog to be very helpful. In that post he also references Albert Rapp’s The ultimate guide to starting a Quarto blog. I’m sure I’ll follow up with lessons learned from this in time."
  },
  {
    "objectID": "posts/fastai-2020-part2/lesson_00/index.html",
    "href": "posts/fastai-2020-part2/lesson_00/index.html",
    "title": "Lesson 00: Exports and Using Jupyter Notebooks as a Development Environment",
    "section": "",
    "text": "#export\nTEST = 'test'\n\nThis is a test variable that will be available in our ./exports/ directory as ./exports/00_exports.ipynb\n\nSave the notebook here before running the next cell. Saving here writes the cells that have been run in this current session to file rather than a temporary file so that our notebook2script.py code can access all the #export cells that we want it to.\n\n# !python notebook2script.py 00_exports.ipynb\n%run notebook2script.py 00_exports.ipynb\n\nConverted 00_exports.ipynb to nb_00.py\n\n\n\n\nCells with the first line of the cell as: “#export” will be picked up by the notebook2script.py script and turned into a repository of exported or saved code that can be loaded and thus persist across the project development environment.\nJupyter notebooks are just json files where each cell is an entry in a list called 'cells'. As seen below, each cell has a number of attributes: cell_type, execution_unit, metadata, outputs, and source. The notebook2script.py script just finds all the cells that have #export as the first line in the cell and then it exports the rest of the code in those cells to a respective file in our exports/ directory.\n\nimport json\nd = json.load(open('00_exports.ipynb', 'r'))['cells']\nd[2]\n\n{'cell_type': 'code',\n 'execution_count': 1,\n 'metadata': {},\n 'outputs': [],\n 'source': ['#export\\n', \"TEST = 'test'\"]}\n\n\n\n\n\nworking-dir/\n            00_exports.ipynb\n            notebook2script.py\n            exports/\n                    nb_00.py\n\n\n\nThe following is the notebook2script code in its entirety.\n\nimport json, fire, re\nfrom pathlib import Path\n\ndef is_export(cell):\n    if cell['cell_type'] != 'code':\n        return False\n    src = cell['source']\n    if len(src) == 0 or len(src[0]) < 7:\n        return False\n    # import pdb; pdb.set_trace()\n    return re.match(pattern=r'^\\s*#\\s*export\\s*$', string=src[0], flags=re.IGNORECASE) is not None\n\ndef notebook2script(fname):\n    fname = Path(fname)\n    fname_out = f'nb_{fname.stem.split(\"_\")[0]}.py'\n    main_dic = json.load(open(fname, 'r'))\n    code_cells = [c for c in main_dic['cells'] if is_export(c)]\n    module = f'''\n#################################################\n### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n#################################################\n# file to edit: dev_nb/{fname.name}\n'''\n    for cell in code_cells:\n        module += ''.join(cell['source'][1:]) + '\\n\\n'\n    # remove trailing spaces\n    module = re.sub(pattern=r' +$', repl='', string=module, flags=re.MULTILINE)\n    open(fname.parent/'exports'/fname_out, 'w').write(module[:-2])\n    print(f\"Converted {fname} to {fname_out}\")\n\nif __name__ == '__main__':\n    fire.Fire(notebook2script)"
  },
  {
    "objectID": "posts/fastai-2020-part2/lesson_01/index.html",
    "href": "posts/fastai-2020-part2/lesson_01/index.html",
    "title": "Lesson 01: Matrix Multiplication",
    "section": "",
    "text": "The foundations we’ll assume throughout this course are: 1. Python 2. PyTorch modules (non-DL) 3. PyTorch indexable tensor, and tensor creation (including RNGs) 4. fastai.datasets (the downloadable datasets)\n\n\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\n#export\nfrom exports.nb_00 import *\nimport operator\n\ndef test(a, b, cmp, cname=None):\n    if cname is None: cname = cmp.__name__\n    assert cmp(a,b), f'{cname}:\\n{a}\\n{b}'\n\ndef test_eq(a, b): test(a, b, operator.eq, '==')\n\n\ntest_eq(TEST, 'test')\n\nTo run tests in console:\n!python run_notebook.py 01_matmul.ipynb\n\n## Get Data\n\nBefore we can work on developing a matrix multiply algorithm we need to get some matricies to multiply.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n#export\nfrom pathlib import Path\nimport os\nimport sys\nimport numpy as np\nfrom IPython.core.debugger import set_trace\n# from fastai.data.core import Datasets\nfrom fastai.data.external import *\nimport pickle, gzip, math, torch, matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom torch import tensor\n\n# MNIST_URL = 'http://deeplearning.net/data/mnist/mnist.pkl'\n:::\n\npath = untar_data(url=URLs.MNIST)\npath\n\nPath('/home/xar/.fastai/data/mnist_png')\n\n\nOur data is split into training and testing directories.\n\nfor d in os.listdir(path):\n    print(d)\n\ntesting\ntraining\n\n\n\n#export\ndef get_mnist_files(path, train_str='training', test_str='testing'):\n    for data_str in (train_str, test_str):\n        if not Path(path/data_str).exists(): sys.exit('Directory not matching given names.')\n        dirs = [path/data_str/d for d in sorted(os.listdir(path/data_str))]\n        names = [[p,f] for d in dirs for (p,dir,f) in os.walk(d)]\n        x_list = np.asarray([plt.imread(Path(p)/f) for (p,fl) in names for f in fl])\n        y_list = [i for i,(p,fl) in enumerate(names) for f in fl]\n        if data_str == train_str:\n            x_train = x_list\n            y_train = y_list\n        elif data_str == test_str:\n            x_test = x_list\n            y_test = y_list\n    return map(tensor, (x_train, y_train, x_test, y_test))\n\n\nx_train, y_train, x_valid, y_valid = get_mnist_files(path=path)\n# x_train, y_train, x_valid, y_valid \n# x_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\n\n\nx_train.shape, x_valid.shape\n\n(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))\n\n\nThe training tensor shapes should be flattened.\n\nx_train = x_train.view(-1, 28*28)\nx_valid = x_valid.view(-1, 28*28)\nx_train.shape, x_valid.shape\n\n(torch.Size([60000, 784]), torch.Size([10000, 784]))\n\n\n\nx_train, y_train\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([0, 0, 0,  ..., 9, 9, 9]))\n\n\n\nn,c = x_train.shape\nprint(f'x_train.shape: {x_train.shape}\\ny_train.shape: {y_train.shape}\\nx_valid.shape: {x_valid.shape}\\ny_valid.shape: {y_valid.shape}')\n\nx_train.shape: torch.Size([60000, 784])\ny_train.shape: torch.Size([60000])\nx_valid.shape: torch.Size([10000, 784])\ny_valid.shape: torch.Size([10000])\n\n\n\nassert n == y_train.shape[0] == 6e4\ntest_eq(c, 28*28)\ntest_eq(y_train.min(), 0)\ntest_eq(y_train.max(), 9)\n\nAssertions are as expected for the mnist dataset. Now plot the data:\n\nmpl.rcParams['image.cmap'] = 'gray'\nimg = x_train[0]\nimg.view(28,28).type()\n\n'torch.FloatTensor'\n\n\n\nplt.imshow(img.view(28,28)) \n\n<matplotlib.image.AxesImage at 0x7f3e760fcf70>\n\n\n\n\n\n\n\n\nweights has to be the size of input by the size of output for a single layer model.\n\nweights = torch.randn(784, 10)\nbias = torch.zeros(10)\n\n\n\n\ndef matmul(a, b):\n    ar, ac = a.shape # n_rows, n_cols\n    br, bc = b.shape\n    assert ac == br # inner dimensions have to match\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): # or br\n                c[i,j] += a[i,k] * b[k,j]\n    return c\n\n\nm1 = x_valid[:5]\nm2 = weights\nm1.shape, m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\ntime1 = %timeit -n 5 -o _=matmul(m1, m2)\n\n463 ms ± 809 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nt1 = matmul(m1, m2)\n\nThis is many times too slow to be useful.\nTo speed this up we need to remove the Python as much as possible, prioritizing the most used or computationally demanding parts. We can pass this computation to something like PyTorch which is much faster. PyTorch is written in a library called A10 so which is much faster so getting our computation down to A10 as much as possible is the goal.\n\n\nOperators (+, -, *, /, <, >, ==) are usually element-wise.\nSome examples of element-wise operations:\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na, b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\n\na + b\n\ntensor([12., 14.,  3.])\n\n\n\n(a < b).float().mean()\n\ntensor(0.6667)\n\n\n\nm = tensor([[1., 2, 3], [4, 5, 6], [7, 8, 9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\nWe can do element-wise operations on rank-2 tensors (matrix) too. Let’s do a Frobenius norm as practice.\nFrobenius norm: $ \\[\\begin{equation}\n    \\lvert\\lvert A \\rvert\\rvert_F = \\left( \\sum_{i,j=1}^N \\lvert a_{ij} \\rvert^2 \\right)^{1/2}\n\\end{equation}\\] $\n\ndef frobenius_norm(m):\n    return (m*m).sum().sqrt()\nfrobenius_norm(m)\n\ntensor(16.8819)\n\n\n\n\n\nThat is enough information to replace the loop over k in matmul.\n\ndef matmul(a, b):\n    ar, ac = a.shape # n_rows, n_cols\n    br, bc = b.shape\n    assert ac == br # inner dimensions have to match\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            c[i,j] = (a[i,:] * b[:,j]).sum()\n    return c\n\n\ntime2 = %timeit -n 100 -o _=matmul(m1, m2)\n\n689 µs ± 29.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nprint(f'A speed up from full-loop matmul of: {time1.average/time2.average:.3e}')\n\nA speed up from full-loop matmul of: 6.730e+02\n\n\nNow lets test that the results of both methods are the same, (or very close since float calculations are slightly different depending on how they’re done).\n\n#export\ndef near(a, b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\ndef test_near(a, b): test(a, b, near)\n\n\ntest_near(t1, matmul(m1, m2))\n\n\n\n\n\n\n\na\n\ntensor([10.,  6., -4.])\n\n\n\na > 0 # broadcasting a against a scalar\n\ntensor([ True,  True, False])\n\n\n\na + 1\n\ntensor([11.,  7., -3.])\n\n\n\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nm*2\n\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n\n\n\n\n\n\nc = tensor([10., 20, 30]); c\n\ntensor([10., 20., 30.])\n\n\n\nm.shape, c.shape\n\n(torch.Size([3, 3]), torch.Size([3]))\n\n\n\nm + c\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\nWe can see what a tensor will look like expanded as the shape of another tensor:\n\nc\n\ntensor([10., 20., 30.])\n\n\n\nc_exp = c.expand_as(m)\nc_exp\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n\n\n\nm + c_exp\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\nDoes the same thing as implicitly broadcasting with c\n\nc_exp.storage()\n\n 10.0\n 20.0\n 30.0\n[torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 3]\n\n\nc_exp.storage() tells us that this is stored as a 3x1 vector even though it looks like a 3x3 matrix.\n\nc_exp.stride(), c_exp.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\nc_exp.shape tells us that c_exp knows that it’s meant to be a 3x3 matrix c_exp.stride() tells us that when it’s going from column to column it should take 1 step in memory (because c was a row-vector), but when it goes from row to row it takes 0 steps in memory, (again because c is a row vector so there is nothing in the next row).\nNow shown for a column vector t that is expanded in the same way:\n\nt = tensor([[10.], [20], [30]]); t\n\ntensor([[10.],\n        [20.],\n        [30.]])\n\n\n\nt_exp = t.expand_as(m)\nt_exp\n\ntensor([[10., 10., 10.],\n        [20., 20., 20.],\n        [30., 30., 30.]])\n\n\n\nt_exp.stride(), t_exp.shape\n\n((1, 0), torch.Size([3, 3]))\n\n\nThe stride values switch from before because t is a column-vector and c was a row-vector.\n\n\n\n\nc, c.shape\n\n(tensor([10., 20., 30.]), torch.Size([3]))\n\n\n\nc.unsqueeze(dim=0), c.unsqueeze(dim=0).shape\n\n(tensor([[10., 20., 30.]]), torch.Size([1, 3]))\n\n\n\nc.unsqueeze(dim=1), c.unsqueeze(dim=1).shape\n\n(tensor([[10.],\n         [20.],\n         [30.]]),\n torch.Size([3, 1]))\n\n\nThe same unsqueezing can be done by indexing with a None\n\nc.shape, c[None,:].shape, c[:,None].shape\n\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n\n\nYou can always skip trailing columns :. And ... means skip all preceding dimentions.\n\nc.shape, c[None].shape, c[...,None].shape\n\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\nm.shape, m[None].shape, m[:,None,None,:].shape, m[...,None].shape, m[None,...,None].shape\n\n(torch.Size([3, 3]),\n torch.Size([1, 3, 3]),\n torch.Size([3, 1, 1, 3]),\n torch.Size([3, 3, 1]),\n torch.Size([1, 3, 3, 1]))\n\n\n\n\n\n\nSo if we are still looping i the first dimension of a in matmul, then what we want to do is unsqueeze(dim=-1) the last dimension so that the last dimension of a gets broadcast to the last dimension of b.\n\nm1.shape, m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nprint(f'So the unsqueezed shape, {m1[0].unsqueeze(-1).shape}, gets expanded as, {m1[0].unsqueeze(-1).expand_as(m2).shape}, for multiplication with m2.')\n\nSo the unsqueezed shape, torch.Size([784, 1]), gets expanded as, torch.Size([784, 10]), for multiplication with m2.\n\n\nThis is an important quality of broadcasting and unsqueeze because broadcasting only works on singleton dimensions. So for example, trying to broadcast shape([784]) to shape([784, 10]) would give an error. The first shape must be shape([784, 1]) to broadcast.\nThen we have to sum over the inner dimension axis, 0.\n\nproduct = m1[0].unsqueeze(-1) * m2\nprint('Before sum:\\t', product.shape)\nproduct = (m1[0].unsqueeze(-1) * m2).sum(0)\nprint('After sum:\\t', product.shape)\n\nBefore sum:  torch.Size([784, 10])\nAfter sum:   torch.Size([10])\n\n\nIn matrix multiplication the first row of the product matrix is the result of multiplying the first row of a with all the columns of b, the product of each column of b gets summed. So when we broadcast a[i] to the entire b we then have to sum over dim=0 to get the row of c our product matrix.\n\ndef matmul(a, b):\n    ar, ac = a.shape # n_rows, n_cols\n    br, bc = b.shape\n    assert ac == br # inner dimensions have to match\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()    # previous\n        c[i]   = (a[i].unsqueeze(-1) * b).sum(dim=0)\n    return c\n\n\ntime3 = %timeit -n 1000 -o _=matmul(m1, m2)\n\n85.3 µs ± 6.12 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nprint(f'A speed up from full-loop matmul by a factor of: {time1.average/time3.average:.3e}')\n\nA speed up from full-loop matmul by a factor of: 5.431e+03\n\n\n\ntest_near(t1, matmul(m1, m2))\n\n\n\n\nWhen operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when: - they are equal, or - one of them is 1, in which case that dimension is broadcasted to make it the same size\nArrays do not need to have the same number of dimensions. For example, if you have a 256,256,3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\nImage  (3d array): 256 x 256 x 3\nScale  (1d array):             3\nResult (3d array): 256 x 256 x 3\nThe numpy documentation includes several examples of what dimensions can and can not be broadcast together.\n\n\n\nc, c.shape\n\n(tensor([10., 20., 30.]), torch.Size([3]))\n\n\n\nc[None,:], c[None,:].shape\n\n(tensor([[10., 20., 30.]]), torch.Size([1, 3]))\n\n\n\nc[:,None], c[:,None].shape\n\n(tensor([[10.],\n         [20.],\n         [30.]]),\n torch.Size([3, 1]))\n\n\n\nc[None,:] * c[:,None] # element-wise multiplication \n\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n\n\n\nc[:,None] * c[None,:] # element-wise multiplication \n\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n\n\nBecause the indicies of these two matricies are either matching or 1 broadcasting gives an interesting result here.\n\n\n\n\nEinstein summation (einsum) is a compact representation for combining products and sums in a general way. From the numpy docs: > The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so np.einsum('i,i', a, b) is equivalent to p.inner(a, b). If a label appears only once, it is not summed, so np.einsum('i', a) produces a view of a with no changes.\n\n# c[i,j] += a[i,k] * b[k,j]             # first matmul \n# c[i,j] = (a[i,:] * b[:,j]).sum()      # second matmul\n#  [i,j] <-  [i,k] *  [k,j]\n#            [i,k] *  [k,j] -> [i,j]\n#             i k  ,   k j  ->  i j \n#             ik,kj -> ij\n\ndef matmul(a, b): return torch.einsum('ik,kj->ij', a, b)\n\n\ntime4 = %timeit -n 1000 -o _=matmul(m1, m2)\n\n21.3 µs ± 4.58 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nprint(f'A speed up from full-loop matmul by a factor of: {time1.average/time4.average:.3e}')\n\nA speed up from full-loop matmul by a factor of: 2.176e+04\n\n\neinsum can be used for lot’s of different things like transposing and other operations even if they don’t include matrix multiplication.\nFor instance, if PyTorch didn’t have batch-wise matrix multiplication we could create it ourselves using the following:\ntorch.einsum('bik,bkj->bij', a, b)\n\n\n\n\ntime5 = %timeit -n 1000 -o _=m1.matmul(m2)\n\n7.79 µs ± 1.28 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nprint(f'A speed up from full-loop matmul by a factor of: {time1.average/time5.average:.3e}')\n\nA speed up from full-loop matmul by a factor of: 5.945e+04\n\n\nMatrix multiplication is so common and useful that it has it’s own symbol in PyTorch, @. So the following cell calls the exact same code as m1.matmul(m2).\n\ntime6 = %timeit -n 1000 -o t2 = m1 @ m2\n\n7.15 µs ± 1.29 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nprint(f'A speed up from full-loop matmul by a factor of: {time1.average/time6.average:.3e}')\n\nA speed up from full-loop matmul by a factor of: 6.477e+04\n\n\n\n\n\n\n\nSave the notebook here before running the next cell.\n\n%run notebook2script.py 01_matmul.ipynb\n\nConverted 01_matmul.ipynb to nb_01.py\n\n\n<Figure size 432x288 with 0 Axes>"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "atticusmatticus.github.io",
    "section": "",
    "text": "Series\n\nfastai 2020 Part 2 “From the Foundations” Series\nThis series follows my journey through the fastai part 2 lessons.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLesson 01: Matrix Multiplication\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nBuilding matrix multiplication efficiently and using exports to save functionality for later.\n\n\n\n\n\n\nFeb 6, 2023\n\n\nMax Mattson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLesson 00: Exports and Using Jupyter Notebooks as a Development Environment\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nHardly a lesson… more of an introduction to the exporting mechanism.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nMax Mattson\n\n\n\n\n\n\n  \n\n\n\n\nHello World\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nMax Mattson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am creating this blog mostly to record my notes and experiences along my path to becoming a proficient data scientist.\nMy background is in chemistry, physics, and math and I have a Ph.D. in theoretical physical chemistry. Because of this I enjoy understanding how systems function on a fundamental level, so there will undoubtably be some asides of that nature sprinkled throughout this blog."
  },
  {
    "objectID": "fastai-2020-part2-series.html",
    "href": "fastai-2020-part2-series.html",
    "title": "Series: fastai-2020-part2",
    "section": "",
    "text": "This is my first fastai “From the Foundations” series. This series is all about building the methods that we’ve been using in part 1 ourselves and learning some of the math and programming tricks that help enable our tools.\n\n\n\n\n\n\n\n\n\n\n\nLesson 01: Matrix Multiplication\n\n\n\n\n\nBuilding matrix multiplication efficiently and using exports to save functionality for later.\n\n\n\n\n\n\nFeb 6, 2023\n\n\nMax Mattson\n\n\n\n\n\n\n\n\nLesson 00: Exports and Using Jupyter Notebooks as a Development Environment\n\n\n\n\n\nHardly a lesson… more of an introduction to the exporting mechanism.\n\n\n\n\n\n\nFeb 3, 2023\n\n\nMax Mattson\n\n\n\n\n\n\nNo matching items"
  }
]